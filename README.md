# Credit Risk AnalysisProject

Este proyecto implementa un sistema completo de anÃ¡lisis de riesgo crediticio. 
Incluye una API REST construida con FastAPI, un motor de predicciÃ³n asincrÃ³nico basado en Redis, 
una base de datos ligera en SQLite, y una interfaz de usuario desarrollada con Streamlit. 
Toda la infraestructura es fÃ¡cilmente desplegable mediante `Docker Compose`.

## How to Run the project

```docker compose up --build ```

## UI

http://localhost:8051

## UI Docs

http://localhost:8051/docs

## Samples Api Request

### get a prediction

```
curl -X POST http://localhost:8000/model/predict \
  -H "Content-Type: application/json" \
  -d '{
    "MARITAL_STATUS": 1,
    "MONTHS_IN_RESIDENCE": 12,
    "AGE": 35,
    "OCCUPATION_TYPE": 1,
    "SEX": "M",
    "FLAG_RESIDENCIAL_PHONE": "Y",
    "STATE_OF_BIRTH": "BA",
    "RESIDENCIAL_STATE": "BA",
    "RESIDENCE_TYPE": 1,
    "PROFESSIONAL_STATE": "BA",
    "PRODUCT": "Personal Loan",
    "RESIDENCIAL_CITY": "BahÃ­a Blanca",
    "RESIDENCIAL_BOROUGH": "Centro",
    "RESIDENCIAL_PHONE_AREA_CODE": "291",
    "RESIDENCIAL_ZIP_3": "800",
    "PROFESSIONAL_ZIP_3": "800"
  }'
```
### get historic of predictions

```
curl -X GET http://localhost:8000/model/predictions 

```


## Estructura del Proyecto

### `credit_risk_analysis/db/`
Contiene los scripts y componentes necesarios para la inicializaciÃ³n e interacciÃ³n con la base de datos. Actualmente se utiliza **SQLite** como motor de almacenamiento relacional, elegido por su simplicidad y compatibilidad con despliegues livianos en AWS (limitaciÃ³n impuesta por la imposibilidad de usar PostgreSQL en este entorno).

- `prediction_orm.py`: Define el modelo ORM para registrar y consultar predicciones realizadas.

---

### `credit_risk_analysis/api/`
Define los **endpoints REST** expuestos por el sistema para interactuar con el motor de predicciÃ³n. A travÃ©s de esta capa se reciben solicitudes externas para generar nuevas predicciones y consultar el historial existente.

- `router.py`: Contiene las rutas y controladores principales para los endpoints del sistema.

---

### `credit_risk_analysis/model/`
Incluye la lÃ³gica encapsulada del **modelo de machine learning**. Esta capa actÃºa como fachada hacia el pipeline de inferencia, cargando el modelo serializado y ejecutando la predicciÃ³n sobre los datos entrantes.

- `predict.py`: FunciÃ³n principal de predicciÃ³n, incluye validaciones y preprocesamiento.

---

### `models/`
Contiene los **artefactos del modelo serializado**, incluyendo tanto el pipeline de preprocesamiento como el modelo entrenado. Estos objetos son utilizados en tiempo de inferencia.

- `preprocessor.pkl`: Pipeline de preprocesamiento de datos.
- `stacking_model.pkl`: Modelo de machine learning serializado (Stacking Ensemble).

---

### `credit_risk_analysis/services/`
Define los **servicios asincrÃ³nicos** encargados de gestionar la cola de trabajos de predicciÃ³n mediante Redis. Este patrÃ³n desacopla el proceso de predicciÃ³n de la interfaz API, permitiendo un escalamiento mÃ¡s flexible.

- `prediction_job_producer.py`: Encola nuevas tareas de predicciÃ³n.
- `prediction_job_consumer.py`: Worker que consume tareas de Redis y ejecuta las predicciones.
- `schema.py`: Define esquemas y tipos usados entre componentes del servicio.

---

### `UI/`
Contiene la **interfaz de usuario desarrollada en Streamlit**, pensada para permitir interacciÃ³n sencilla con el sistema por parte de usuarios no tÃ©cnicos. Se pueden realizar predicciones manuales y visualizar resultados.

- `app.py`: Archivo principal de la aplicaciÃ³n Streamlit.
- `assets/`: Archivos estÃ¡ticos (CSS, imÃ¡genes, etc.)
- `logo_intro.css`: Estilos personalizados de la interfaz.

---

### `aws/`
Scripts de automatizaciÃ³n y configuraciÃ³n para despliegue del sistema en una instancia EC2 de AWS.

- `connect_aws.sh`: ConexiÃ³n por SSH a la instancia remota.
- `setup_env_aws.sh`: Configura el entorno en la instancia EC2 (Docker, dependencias, etc.).
- `subir_proyecto_aws.sh`: Sincroniza el proyecto local con la instancia EC2.

---

## Despliegue

Todo el sistema estÃ¡ **dockerizado**. La ejecuciÃ³n completa de los servicios se realiza mediante:

```bash
docker compose up --build -d
```

Este comando levanta los siguientes contenedores:

- API REST
- Redis
- Base de datos SQLite
- Interfaz Streamlit

> âš ï¸ Nota: Debido a restricciones de red y almacenamiento en el entorno AWS Free Tier, se utiliza SQLite en lugar de PostgreSQL para simplificar el despliegue.

---

## TecnologÃ­as utilizadas

- Python 3.10+
- FastAPI
- Redis
- Streamlit
- Scikit-learn
- Docker / Docker Compose
- SQLite

---

## Estado del Proyecto

âœ… Funcional  
ğŸ”œ En futuras versiones se evaluarÃ¡ migrar la base de datos a PostgreSQL y escalar el sistema con Kubernetes para ambientes productivos.

---