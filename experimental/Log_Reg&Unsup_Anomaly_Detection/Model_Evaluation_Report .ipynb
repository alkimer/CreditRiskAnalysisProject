{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbbd7874",
   "metadata": {},
   "source": [
    "# Supervised Classification and Unsupervised Anomaly Detection Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5495350b",
   "metadata": {},
   "source": [
    "_Author: Maria Laura Borra - Date: 6/27/2025_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0bbb98",
   "metadata": {},
   "source": [
    "##  1. Objective\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55672662",
   "metadata": {},
   "source": [
    "The goal of this notebook is to develop and evaluate various models to identify individuals who are at high risk of defaulting on credit payments. This involves comparing both unsupervised anomaly detection methods, which detect unusual behavior patterns without labeled examples of defaults, and supervised classification models, which leverage labeled data to predict the likelihood of default.\n",
    "\n",
    "By systematically assessing the performance of these models using metrics such as ROC AUC, precision, and recall, we aim to select the most effective approach for accurately flagging high-risk customers and thereby improve credit risk management.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0adda60f",
   "metadata": {},
   "source": [
    "## 2. Dataset Description\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfe6c41",
   "metadata": {},
   "source": [
    "The dataset underwent several preprocessing steps to ensure quality and compatibility with machine learning models:\n",
    "\n",
    "- Loading and Cleaning: Raw text data was loaded and its columns were renamed using a provided mapping. A custom cleaning function addressed missing values, normalized text, dropped irrelevant columns, and engineered new features.\n",
    "\n",
    "- Train/Validation/Test Split: Data was split into training, validation, and test sets to ensure unbiased model evaluation.\n",
    "\n",
    "- Preprocessing Pipeline:\n",
    "\n",
    "    1- Imputation: Median strategy for numeric features, most frequent for categorical.\n",
    "   \n",
    "    2- Feature Encoding: Applied encoding for categorical variables with controlled cardinality.\n",
    "   \n",
    "    3- Scaling: MinMaxScaler was applied\n",
    "\n",
    "    4- Class Imbalance was > 70% but No SMOTE applied due to low performance detected on model training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0af234c",
   "metadata": {},
   "source": [
    "### 2.1 Importing the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10eea03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "# Cambiar al directorio del proyecto\n",
    "import os\n",
    "os.chdir(\"/content/drive/My Drive/Learning/AnyoneAI/Final_Project/Models\")  # change to your actual project root folder\n",
    "print(\"Current dir:\", os.getcwd())  # confirm\n",
    "# Instalar dependencias desde requirements.txt\n",
    "!pip install -r \"/content/drive/My Drive/Learning/AnyoneAI/Final_Project/Models/requirements.txt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fe821ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Administrator\\Desktop\\CreditRiskAnalysisProject\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <ins><a href=\"https://ydata.ai/register\">Upgrade to ydata-sdk</a></ins>\n",
       "                <p>\n",
       "                    Improve your data and profiling with ydata-sdk, featuring data quality scoring, redundancy detection, outlier identification, text validation, and synthetic data generation.\n",
       "                </p>\n",
       "            </div>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-06-28 19:07:08.868\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.config\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mPROJ_ROOT path is: C:\\Users\\Administrator\\Desktop\\CreditRiskAnalysisProject\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import glob\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Third-party library imports\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import pandas as pd\n",
    "from ydata_profiling import ProfileReport\n",
    "\n",
    "# Project-specific path setup\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "sys.path.append(project_root)\n",
    "\n",
    "# Project imports - Configs\n",
    "from src.config import ( RAW_DATA_DIR,EXTERNAL_DATA_DIR)\n",
    "\n",
    "# Project imports - Data utilities\n",
    "from src.data.data_utils import (\n",
    "    get_feature_target,\n",
    "    get_train_val_sets,\n",
    "    df_to_csv,\n",
    "    summarize_column_counts,\n",
    "    load_txt_with_mapped_columns,\n",
    ")\n",
    "from src.data.cleaning_dataset import clean_dataset\n",
    "\n",
    "# Project imports - Preprocessing\n",
    "from src.preprocessing.preprocessing import simple_preprocess\n",
    "\n",
    "# Project imports - Modeling\n",
    "from src.modeling.models import (\n",
    "    train_classification_pipeline,\n",
    "    run_isolation_forest,\n",
    "    run_one_class_svm,\n",
    ")\n",
    "#Modeling\n",
    "from sklearn.metrics import classification_report, roc_auc_score, precision_score, recall_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db9b750",
   "metadata": {},
   "source": [
    "### 2.2 Collect the Data, convert and clean DF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f155bca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting dataset cleaning...\n",
      "Normalized string columns (stripped and uppercased): ['CLERK_TYPE', 'APPLICATION_SUBMISSION_TYPE', 'SEX', 'STATE_OF_BIRTH', 'CITY_OF_BIRTH', 'RESIDENCIAL_STATE', 'RESIDENCIAL_CITY', 'RESIDENCIAL_BOROUGH', 'FLAG_RESIDENCIAL_PHONE', 'RESIDENCIAL_PHONE_AREA_CODE', 'FLAG_MOBILE_PHONE', 'COMPANY', 'PROFESSIONAL_STATE', 'PROFESSIONAL_CITY', 'PROFESSIONAL_BOROUGH', 'FLAG_PROFESSIONAL_PHONE', 'PROFESSIONAL_PHONE_AREA_CODE', 'FLAG_ACSP_RECORD', 'RESIDENCIAL_ZIP_3', 'PROFESSIONAL_ZIP_3']\n",
      "Numeric columns changed to Category: ['PAYMENT_DAY', 'POSTAL_ADDRESS_TYPE', 'MARITAL_STATUS', 'EDUCATION_LEVEL', 'NACIONALITY', 'FLAG_VISA', 'FLAG_MASTERCARD', 'FLAG_DINERS', 'FLAG_AMERICAN_EXPRESS', 'FLAG_OTHER_CARDS', 'RESIDENCE_TYPE', 'PROFESSION_CODE', 'OCCUPATION_TYPE', 'MATE_PROFESSION_CODE', 'PRODUCT', 'RESIDENCIAL_ZIP_3', 'PROFESSIONAL_ZIP_3', 'FLAG_INCOME_PROOF', 'FLAG_CPF', 'FLAG_RG', 'FLAG_HOME_ADDRESS_DOCUMENT', 'FLAG_EMAIL', 'FLAG_RESIDENCIAL_PHONE', 'FLAG_MOBILE_PHONE', 'EDUCATION_LEVEL.1']\n",
      "Created column 'TOTAL_CREDIT_CARDS' by counting 1s in: ['FLAG_VISA', 'FLAG_MASTERCARD', 'FLAG_DINERS', 'FLAG_AMERICAN_EXPRESS', 'FLAG_OTHER_CARDS']\n",
      "Created column 'DOC_CONFIRMATION' by counting 1s in: ['FLAG_INCOME_PROOF', 'FLAG_CPF', 'FLAG_RG', 'FLAG_HOME_ADDRESS_DOCUMENT', 'FLAG_EMAIL']\n",
      "Created column 'CONTACTABILITY' by counting 1s in: ['FLAG_RESIDENCIAL_PHONE', 'FLAG_MOBILE_PHONE', 'FLAG_PROFESSIONAL_PHONE', 'FLAG_EMAIL']\n",
      "Created column 'IS_NOT_FOREIGNER' by counting 1s in: ['NACIONALITY']\n",
      "Created column 'HAS_PREMIUM_CARD' by counting 1s in: ['FLAG_DINERS', 'FLAG_AMERICAN_EXPRESS']\n",
      "Created column 'HAS_ASSETS&HAS_PREMIUM_CARD' by counting 1s in: ['QUANT_CARS', 'PERSONAL_ASSETS_VALUE', 'FLAG_DINERS', 'FLAG_AMERICAN_EXPRESS']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Administrator\\Desktop\\CreditRiskAnalysisProject\\src\\data\\cleaning_dataset.py:108: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  temp = df[cols_to_count].replace({'Y': 1, 'N': 0})\n",
      "c:\\Users\\Administrator\\Desktop\\CreditRiskAnalysisProject\\src\\data\\cleaning_dataset.py:108: FutureWarning: The behavior of Series.replace (and DataFrame.replace) with CategoricalDtype is deprecated. In a future version, replace will only be used for cases that preserve the categories. To change the categories, use ser.cat.rename_categories instead.\n",
      "  temp = df[cols_to_count].replace({'Y': 1, 'N': 0})\n",
      "c:\\Users\\Administrator\\Desktop\\CreditRiskAnalysisProject\\src\\data\\cleaning_dataset.py:108: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  temp = df[cols_to_count].replace({'Y': 1, 'N': 0})\n",
      "c:\\Users\\Administrator\\Desktop\\CreditRiskAnalysisProject\\src\\data\\cleaning_dataset.py:108: FutureWarning: The behavior of Series.replace (and DataFrame.replace) with CategoricalDtype is deprecated. In a future version, replace will only be used for cases that preserve the categories. To change the categories, use ser.cat.rename_categories instead.\n",
      "  temp = df[cols_to_count].replace({'Y': 1, 'N': 0})\n",
      "c:\\Users\\Administrator\\Desktop\\CreditRiskAnalysisProject\\src\\data\\cleaning_dataset.py:108: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  temp = df[cols_to_count].replace({'Y': 1, 'N': 0})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created column 'NO_CREDIT_HISTORY' by counting 0s in: ['FLAG_VISA', 'FLAG_MASTERCARD', 'FLAG_DINERS', 'FLAG_AMERICAN_EXPRESS', 'FLAG_OTHER_CARDS', 'QUANT_BANKING_ACCOUNTS', 'QUANT_SPECIAL_BANKING_ACCOUNTS']\n",
      "Created column 'TOTAL_INCOME' by summing: ['OTHER_INCOMES', 'PERSONAL_MONTHLY_INCOME']\n",
      "Created column 'TOTAL_BANK_ACCOUNTS' by summing: ['QUANT_BANKING_ACCOUNTS', 'QUANT_SPECIAL_BANKING_ACCOUNTS']\n",
      "Created column 'STABILITY_INDEX' by summing: ['MONTHS_IN_RESIDENCE', 'MONTHS_IN_THE_JOB']\n",
      "Created ratio column 'MONTHS_IN_THE_JOB_DIV_AGE' as MONTHS_IN_THE_JOB / AGE\n",
      "Created ratio column 'TOTAL_INCOME_DIV_QUANT_DEPENDANTS' as TOTAL_INCOME / QUANT_DEPENDANTS\n",
      "Created ratio column 'PERSONAL_MONTHLY_INCOME_DIV_OTHER_INCOMES' as PERSONAL_MONTHLY_INCOME / OTHER_INCOMES\n",
      "Created ratio column 'TOTAL_INCOME_DIV_AGE' as TOTAL_INCOME / AGE\n",
      "Created ratio column 'TOTAL_INCOME_DIV_TOTAL_BANK_ACCOUNTS' as TOTAL_INCOME / TOTAL_BANK_ACCOUNTS\n",
      "Created ratio column 'TOTAL_INCOME_DIV_TOTAL_CREDIT_CARDS' as TOTAL_INCOME / TOTAL_CREDIT_CARDS\n",
      "Created ratio column 'MONTHS_IN_THE_JOB_DIV_MONTHS_IN_RESIDENCE' as MONTHS_IN_THE_JOB / MONTHS_IN_RESIDENCE\n",
      "Created ratio column 'MONTHS_IN_RESIDENCE_DIV_AGE' as MONTHS_IN_RESIDENCE / AGE\n",
      "Created ratio column 'TOTAL_CREDIT_CARDS_DIV_TOTAL_INCOME' as TOTAL_CREDIT_CARDS / TOTAL_INCOME\n",
      "Created ratio column 'PERSONAL_ASSETS_VALUE_DIV_TOTAL_CREDIT_CARDS' as PERSONAL_ASSETS_VALUE / TOTAL_CREDIT_CARDS\n",
      "Created column 'STATE_OF_BIRTH_EQ_RESIDENCIAL_STATE' to indicate equality between 'STATE_OF_BIRTH' and 'RESIDENCIAL_STATE'\n",
      "Created column 'CITY_OF_BIRTH_EQ_RESIDENCIAL_CITY' to indicate equality between 'CITY_OF_BIRTH' and 'RESIDENCIAL_CITY'\n",
      "Created column 'PROFESSIONAL_CITY_EQ_RESIDENCIAL_CITY' to indicate equality between 'PROFESSIONAL_CITY' and 'RESIDENCIAL_CITY'\n",
      "Created column 'PROFESSIONAL_STATE_EQ_RESIDENCIAL_STATE' to indicate equality between 'PROFESSIONAL_STATE' and 'RESIDENCIAL_STATE'\n",
      "Created column 'RESIDENCIAL_BOROUGH_EQ_PROFESSIONAL_BOROUGH' to indicate equality between 'RESIDENCIAL_BOROUGH' and 'PROFESSIONAL_BOROUGH'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Administrator\\Desktop\\CreditRiskAnalysisProject\\src\\data\\cleaning_dataset.py:163: FutureWarning: The behavior of Series.replace (and DataFrame.replace) with CategoricalDtype is deprecated. In a future version, replace will only be used for cases that preserve the categories. To change the categories, use ser.cat.rename_categories instead.\n",
      "  df.replace([\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardized missing values to NaN.\n",
      "Dropped columns: ['QUANT_ADDITIONAL_CARDS', 'FLAG_RG', 'EDUCATION_LEVEL', 'FLAG_MOBILE_PHONE', 'FLAG_ACSP_RECORD', 'CLERK_TYPE', 'FLAG_CPF', 'FLAG_HOME_ADDRESS_DOCUMENT', 'FLAG_INCOME_PROOF']\n",
      "Renamed column 'TARGET_LABEL_BAD=1' to 'TARGET'\n",
      "Dropped 0 duplicate rows.\n",
      "Dropped likely ID columns: ['ID_CLIENT']\n",
      "Dataset cleaning completed.\n"
     ]
    }
   ],
   "source": [
    "# Define the path to the raw data text file\n",
    "txt_file_path = str(RAW_DATA_DIR / \"PAKDD2010_Modeling_Data.txt\")\n",
    "\n",
    "# Define the path to the Excel file that contains the variable (column) name mappings\n",
    "variables_path = str(EXTERNAL_DATA_DIR / 'PAKDD2010_VariablesList.xls')\n",
    "\n",
    "# Load the raw text data and rename its columns using the mapping from the Excel file\n",
    "# The function 'load_txt_with_mapped_columns'  reads the data and replaces column headers \n",
    "# based on the mapping provided in the Excel file\n",
    "df = load_txt_with_mapped_columns(txt_file_path, variables_path)\n",
    "\n",
    "# Clean the dataset using a custom function\n",
    "# This includes normalizing text, handling missing values, dropping irrelevant columns, \n",
    "# creating new features (counts, sums, ratios), and renaming target column.\n",
    "df = clean_dataset(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39b0ba6",
   "metadata": {},
   "source": [
    "### 2.3 Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cab8f687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Separación realizada correctamente.\n",
      "x_train shape: (50000, 66)\n",
      "y_train shape: (50000,)\n",
      "x_test shape: (50000, 66)\n",
      "y_test shape: (50000,)\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train, x_test, y_test = get_feature_target(df, df, target_column=\"TARGET\")\n",
    "print(\" Separación realizada correctamente.\")\n",
    "print(f\"x_train shape: {x_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"x_test shape: {x_test.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37fe963f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " División en train/val realizada correctamente.\n",
      "x_train shape: (40000, 66)\n",
      "y_train shape: (40000,)\n",
      "x_val shape:   (10000, 66)\n",
      "y_val shape:   (10000,)\n"
     ]
    }
   ],
   "source": [
    "x_train, x_val, y_train, y_val = get_train_val_sets(x_train,y_train) \n",
    "print(\" División en train/val realizada correctamente.\")\n",
    "print(f\"x_train shape: {x_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"x_val shape:   {x_val.shape}\")\n",
    "print(f\"y_val shape:   {y_val.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36b09f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped columns: ['POSTAL_ADDRESS_TYPE', 'FLAG_DINERS', 'FLAG_AMERICAN_EXPRESS', 'FLAG_OTHER_CARDS', 'MONTHS_IN_THE_JOB', 'HAS_PREMIUM_CARD', 'MONTHS_IN_THE_JOB_DIV_AGE']\n",
      "[Step 1] Numeric columns imputed: ['QUANT_DEPENDANTS', 'MONTHS_IN_RESIDENCE', 'PERSONAL_MONTHLY_INCOME', 'OTHER_INCOMES', 'QUANT_BANKING_ACCOUNTS', 'QUANT_SPECIAL_BANKING_ACCOUNTS', 'PERSONAL_ASSETS_VALUE', 'QUANT_CARS', 'AGE', 'TOTAL_CREDIT_CARDS', 'DOC_CONFIRMATION', 'CONTACTABILITY', 'IS_NOT_FOREIGNER', 'HAS_ASSETS&HAS_PREMIUM_CARD', 'NO_CREDIT_HISTORY', 'TOTAL_INCOME', 'TOTAL_BANK_ACCOUNTS', 'STABILITY_INDEX', 'TOTAL_INCOME_DIV_QUANT_DEPENDANTS', 'PERSONAL_MONTHLY_INCOME_DIV_OTHER_INCOMES', 'TOTAL_INCOME_DIV_AGE', 'TOTAL_INCOME_DIV_TOTAL_BANK_ACCOUNTS', 'TOTAL_INCOME_DIV_TOTAL_CREDIT_CARDS', 'MONTHS_IN_THE_JOB_DIV_MONTHS_IN_RESIDENCE', 'MONTHS_IN_RESIDENCE_DIV_AGE', 'TOTAL_CREDIT_CARDS_DIV_TOTAL_INCOME', 'PERSONAL_ASSETS_VALUE_DIV_TOTAL_CREDIT_CARDS']\n",
      "[Custom Feature] INCOME_BELOW_AVG added using avg from training set: 920.30\n",
      "[Step 2] Low-card categorical columns imputed:['PAYMENT_DAY', 'APPLICATION_SUBMISSION_TYPE', 'MARITAL_STATUS', 'NACIONALITY', 'FLAG_RESIDENCIAL_PHONE', 'RESIDENCE_TYPE', 'FLAG_EMAIL', 'FLAG_VISA', 'FLAG_MASTERCARD', 'COMPANY', 'FLAG_PROFESSIONAL_PHONE', 'OCCUPATION_TYPE', 'MATE_PROFESSION_CODE', 'PRODUCT', 'STATE_OF_BIRTH_EQ_RESIDENCIAL_STATE', 'CITY_OF_BIRTH_EQ_RESIDENCIAL_CITY', 'PROFESSIONAL_CITY_EQ_RESIDENCIAL_CITY', 'PROFESSIONAL_STATE_EQ_RESIDENCIAL_STATE', 'RESIDENCIAL_BOROUGH_EQ_PROFESSIONAL_BOROUGH']\n",
      "[Step 2] Low-card categorical columns one-hot encoded:['PAYMENT_DAY', 'APPLICATION_SUBMISSION_TYPE', 'MARITAL_STATUS', 'NACIONALITY', 'FLAG_RESIDENCIAL_PHONE', 'RESIDENCE_TYPE', 'FLAG_EMAIL', 'FLAG_VISA', 'FLAG_MASTERCARD', 'COMPANY', 'FLAG_PROFESSIONAL_PHONE', 'OCCUPATION_TYPE', 'MATE_PROFESSION_CODE', 'PRODUCT', 'STATE_OF_BIRTH_EQ_RESIDENCIAL_STATE', 'CITY_OF_BIRTH_EQ_RESIDENCIAL_CITY', 'PROFESSIONAL_CITY_EQ_RESIDENCIAL_CITY', 'PROFESSIONAL_STATE_EQ_RESIDENCIAL_STATE', 'RESIDENCIAL_BOROUGH_EQ_PROFESSIONAL_BOROUGH']\n",
      "[Step 3] High-card categorical columns imputed:['STATE_OF_BIRTH', 'CITY_OF_BIRTH', 'RESIDENCIAL_STATE', 'RESIDENCIAL_CITY', 'RESIDENCIAL_BOROUGH', 'RESIDENCIAL_PHONE_AREA_CODE', 'PROFESSIONAL_STATE', 'PROFESSIONAL_CITY', 'PROFESSIONAL_BOROUGH', 'PROFESSIONAL_PHONE_AREA_CODE', 'PROFESSION_CODE', 'RESIDENCIAL_ZIP_3', 'PROFESSIONAL_ZIP_3']\n",
      "[Step 3] High-card categorical columns target encoded: ['STATE_OF_BIRTH', 'CITY_OF_BIRTH', 'RESIDENCIAL_STATE', 'RESIDENCIAL_CITY', 'RESIDENCIAL_BOROUGH', 'RESIDENCIAL_PHONE_AREA_CODE', 'PROFESSIONAL_STATE', 'PROFESSIONAL_CITY', 'PROFESSIONAL_BOROUGH', 'PROFESSIONAL_PHONE_AREA_CODE', 'PROFESSION_CODE', 'RESIDENCIAL_ZIP_3', 'PROFESSIONAL_ZIP_3']\n",
      "[Step 1a] Automatically log-transformed columns: ['PERSONAL_MONTHLY_INCOME', 'TOTAL_INCOME', 'TOTAL_INCOME_DIV_QUANT_DEPENDANTS', 'PERSONAL_MONTHLY_INCOME_DIV_OTHER_INCOMES', 'TOTAL_INCOME_DIV_AGE', 'TOTAL_INCOME_DIV_TOTAL_BANK_ACCOUNTS', 'TOTAL_INCOME_DIV_TOTAL_CREDIT_CARDS']\n",
      "[Step 5] Scaled columns: ['QUANT_DEPENDANTS', 'MONTHS_IN_RESIDENCE', 'PERSONAL_MONTHLY_INCOME', 'OTHER_INCOMES', 'PERSONAL_ASSETS_VALUE', 'AGE', 'TOTAL_INCOME', 'STABILITY_INDEX', 'PERSONAL_MONTHLY_INCOME_DIV_OTHER_INCOMES', 'TOTAL_INCOME_DIV_TOTAL_BANK_ACCOUNTS', 'TOTAL_INCOME_DIV_TOTAL_CREDIT_CARDS', 'MONTHS_IN_THE_JOB_DIV_MONTHS_IN_RESIDENCE', 'MONTHS_IN_RESIDENCE_DIV_AGE', 'TOTAL_CREDIT_CARDS_DIV_TOTAL_INCOME', 'PERSONAL_ASSETS_VALUE_DIV_TOTAL_CREDIT_CARDS']\n",
      "Train: (40000, 114) (40000,)\n",
      "Validation: (10000, 114) (10000,)\n",
      "Test: (50000, 114) (50000,)\n"
     ]
    }
   ],
   "source": [
    "(train_x, train_y), (val_x, val_y), (test_x, test_y) = simple_preprocess(\n",
    "    x_train, x_val, x_test,\n",
    "    y_train, y_val, y_test,\n",
    "    numeric_imputer_strategy='median', #Median imputation is appropriate when the distribution of the data is skewed\n",
    "    categorical_imputer_strategy='most_frequent',\n",
    "    apply_smote=False,\n",
    "    cardinality_amount= 17,\n",
    "    threshold_imbalanced=0.99,\n",
    "    #sampling_strategy =  \"auto\",\n",
    ")\n",
    "print(\"Train:\", train_x.shape, train_y.shape)\n",
    "print(\"Validation:\", val_x.shape, val_y.shape)\n",
    "print(\"Test:\", test_x.shape, test_y.shape) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f8efbe",
   "metadata": {},
   "source": [
    "## 3. Models Evaluated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cdbe2e3",
   "metadata": {},
   "source": [
    "The models were developed using a pipeline that included feature selection and hyperparameter tuning. Feature selection was performed with SelectKBest. Hyperparameter optimization was carried out using GridSearchCV to sistematically explore parameter combinations and identify the best configuration for each model.\n",
    "Three models were developed and evaluated to detect high-risk credit customers:\n",
    "\n",
    "- Logistic Regression (Supervised) : A classical supervised model used to predict the probability of default.\n",
    "\n",
    "- Isolation Forest (Unsupervised): Detects anomalies without needing labeled data.\n",
    "\n",
    "- One-Class SVM (Unsupervised): Learns the boundary of normal data; anything outside is flagged as an anomaly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7e2208",
   "metadata": {},
   "source": [
    "### 3.1 Isolation Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea564600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Isolation Forest Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.89      0.81      7392\n",
      "           1       0.23      0.09      0.13      2608\n",
      "\n",
      "    accuracy                           0.68     10000\n",
      "   macro avg       0.48      0.49      0.47     10000\n",
      "weighted avg       0.60      0.68      0.63     10000\n",
      "\n",
      "ROC AUC: 0.5000\n"
     ]
    }
   ],
   "source": [
    "model, y_pred, auc_if, prec_if, rec_if = run_isolation_forest(train_x, train_y, val_x, val_y,contamination=0.1,max_features=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5c859d",
   "metadata": {},
   "source": [
    "### 3.1 One-Class SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ac6ce94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-Class SVM Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.90      0.81      7392\n",
      "           1       0.28      0.11      0.15      2608\n",
      "\n",
      "    accuracy                           0.69     10000\n",
      "   macro avg       0.51      0.50      0.48     10000\n",
      "weighted avg       0.62      0.69      0.64     10000\n",
      "\n",
      "ROC AUC: 0.5071\n"
     ]
    }
   ],
   "source": [
    "model_, y_pred, auc_svm, prec_svm, rec_svm = run_one_class_svm(train_x, train_y, val_x, val_y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c696488",
   "metadata": {},
   "source": [
    "### 3.3 Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4328f6c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.58      0.67      7392\n",
      "           1       0.33      0.58      0.42      2608\n",
      "\n",
      "    accuracy                           0.58     10000\n",
      "   macro avg       0.56      0.58      0.54     10000\n",
      "weighted avg       0.67      0.58      0.60     10000\n",
      "\n",
      "ROC AUC: 0.6127\n"
     ]
    }
   ],
   "source": [
    "model, y_pred,auc_lr, prec_lr, rec_lr = train_classification_pipeline(train_x, train_y, val_x, val_y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68c6cd1",
   "metadata": {},
   "source": [
    "## 4. Comparison Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c370d79",
   "metadata": {},
   "source": [
    "The models were compared using ROC AUC, Precision, and Recall metrics on the validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50b02373",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>ROC AUC</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Notes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Isolation Forest</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.2274</td>\n",
       "      <td>0.0897</td>\n",
       "      <td>Good unsupervised baseline</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>One-Class SVM</td>\n",
       "      <td>0.5071</td>\n",
       "      <td>0.2764</td>\n",
       "      <td>0.1074</td>\n",
       "      <td>More sensitive to scaling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.6127</td>\n",
       "      <td>0.3272</td>\n",
       "      <td>0.5824</td>\n",
       "      <td>Supervised, best performance</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Model  ROC AUC  Precision  Recall  \\\n",
       "0     Isolation Forest   0.5000     0.2274  0.0897   \n",
       "1        One-Class SVM   0.5071     0.2764  0.1074   \n",
       "2  Logistic Regression   0.6127     0.3272  0.5824   \n",
       "\n",
       "                          Notes  \n",
       "0    Good unsupervised baseline  \n",
       "1     More sensitive to scaling  \n",
       "2  Supervised, best performance  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_results = []\n",
    "\n",
    "# Run Isolation Forest\n",
    "#_, _, auc_if, prec_if, rec_if = run_isolation_forest(train_x, train_y, val_x, val_y)\n",
    "model_results.append({\n",
    "    'Model': 'Isolation Forest',\n",
    "    'ROC AUC': round(auc_if, 4),\n",
    "    'Precision': round(prec_if, 4),\n",
    "    'Recall': round(rec_if, 4),\n",
    "    'Notes': 'Good unsupervised baseline'\n",
    "})\n",
    "\n",
    "# Run One-Class SVM\n",
    "#_, _, auc_svm, prec_svm, rec_svm = run_one_class_svm(train_x, train_y, val_x, val_y)\n",
    "model_results.append({\n",
    "    'Model': 'One-Class SVM',\n",
    "    'ROC AUC': round(auc_svm, 4),\n",
    "    'Precision': round(prec_svm, 4),\n",
    "    'Recall': round(rec_svm, 4),\n",
    "    'Notes': 'More sensitive to scaling'\n",
    "})\n",
    "\n",
    "# Run Logistic Regression\n",
    "#_, _, auc_lr, prec_lr, rec_lr = train_classification_pipeline(train_x, train_y, val_x, val_y)\n",
    "model_results.append({\n",
    "    'Model': 'Logistic Regression',\n",
    "    'ROC AUC': round(auc_lr, 4),\n",
    "    'Precision': round(prec_lr, 4),\n",
    "    'Recall': round(rec_lr, 4),\n",
    "    'Notes': 'Supervised, best performance'\n",
    "})\n",
    "\n",
    "results_df = pd.DataFrame(model_results)\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c5e061",
   "metadata": {},
   "source": [
    "## 5. Final Model Selection & try on Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd343d8",
   "metadata": {},
   "source": [
    "After evaluating all models, Logistic Regression was selected as the final model due to its superior performance on validation metrics and its interpretability.\n",
    "\n",
    "The selected model was then tested on a separate test set, confirming its generalization ability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13fbdf59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.58      0.67      7392\n",
      "           1       0.33      0.58      0.42      2608\n",
      "\n",
      "    accuracy                           0.58     10000\n",
      "   macro avg       0.56      0.58      0.54     10000\n",
      "weighted avg       0.67      0.58      0.60     10000\n",
      "\n",
      "ROC AUC: 0.6127\n",
      "Final Evaluation on Test Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.58      0.67     36959\n",
      "           1       0.33      0.59      0.42     13041\n",
      "\n",
      "    accuracy                           0.58     50000\n",
      "   macro avg       0.57      0.59      0.55     50000\n",
      "weighted avg       0.68      0.58      0.61     50000\n",
      "\n",
      "Test ROC AUC: 0.6187\n"
     ]
    }
   ],
   "source": [
    "final_model,y_pred,auc_lr, prec_lr, rec_lr = train_classification_pipeline(train_x, train_y, val_x, val_y)\n",
    "\n",
    "# Now test on a separate test set\n",
    "y_test_pred = final_model.predict(test_x)\n",
    "y_test_prob = final_model.predict_proba(test_x)[:, 1]\n",
    "\n",
    "print(\"Final Evaluation on Test Set:\")\n",
    "print(classification_report(test_y, y_test_pred))\n",
    "print(f\"Test ROC AUC: {roc_auc_score(test_y, y_test_prob):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832ca648",
   "metadata": {},
   "source": [
    "## 6. Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f333f959",
   "metadata": {},
   "source": [
    "- Additional machine learning models will be tested to further evaluate performance across different metrics.\n",
    "\n",
    "- Once the best-performing model is selected, it will be fine-tuned.\n",
    "\n",
    "- The selected model will then be prepared for deployment in a production enviroment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
